{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44423,
     "status": "ok",
     "timestamp": 1658463444355,
     "user": {
      "displayName": "집사위대한",
      "userId": "17063387912399687686"
     },
     "user_tz": -540
    },
    "id": "3LgbCW9Fayfp",
    "outputId": "60e23b92-35ef-4267-e0ce-3184452e75bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.3.0-py3-none-any.whl (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 5.2 MB/s \n",
      "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
      "  Downloading urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 54.5 MB/s \n",
      "\u001b[?25hCollecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.21.0-py3-none-any.whl (358 kB)\n",
      "\u001b[K     |████████████████████████████████| 358 kB 49.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting cryptography>=1.3.4\n",
      "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 42.7 MB/s \n",
      "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
      "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 6.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
      "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.10 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Successfully installed async-generator-1.10 cryptography-37.0.4 h11-0.13.0 outcome-1.2.0 pyOpenSSL-22.0.0 selenium-4.3.0 sniffio-1.2.0 trio-0.21.0 trio-websocket-0.9.2 urllib3-1.26.10 wsproto-1.1.0\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
      "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
      "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [817 kB]\n",
      "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [85.6 kB]\n",
      "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,075 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,063 kB]\n",
      "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,063 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,901 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,304 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,527 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,333 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,105 kB]\n",
      "Fetched 16.5 MB in 6s (2,974 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-460\n",
      "Use 'apt autoremove' to remove it.\n",
      "The following additional packages will be installed:\n",
      "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
      "Suggested packages:\n",
      "  webaccounts-chromium-extension unity-chromium-extension\n",
      "The following NEW packages will be installed:\n",
      "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
      "  chromium-codecs-ffmpeg-extra\n",
      "0 upgraded, 4 newly installed, 0 to remove and 71 not upgraded.\n",
      "Need to get 89.8 MB of archives.\n",
      "After this operation, 302 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 101.0.4951.64-0ubuntu0.18.04.1 [1,142 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 101.0.4951.64-0ubuntu0.18.04.1 [78.5 MB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 101.0.4951.64-0ubuntu0.18.04.1 [4,980 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 101.0.4951.64-0ubuntu0.18.04.1 [5,153 kB]\n",
      "Fetched 89.8 MB in 4s (23.9 MB/s)\n",
      "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
      "(Reading database ... 155653 files and directories currently installed.)\n",
      "Preparing to unpack .../chromium-codecs-ffmpeg-extra_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package chromium-browser.\n",
      "Preparing to unpack .../chromium-browser_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package chromium-browser-l10n.\n",
      "Preparing to unpack .../chromium-browser-l10n_101.0.4951.64-0ubuntu0.18.04.1_all.deb ...\n",
      "Unpacking chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package chromium-chromedriver.\n",
      "Preparing to unpack .../chromium-chromedriver_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
      "Setting up chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
      "Setting up chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
      "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
      "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
      "Setting up chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
      "Setting up chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
      "\n",
      "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting beautifulSoup\n",
      "  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/40/f2/6c9f2f3e696ee6a1fb0e4d7850617e224ed2b0b1e872110abffeca2a09d4/BeautifulSoup-3.2.2.tar.gz#sha256=a04169602bff6e3138b1259dbbf491f5a27f9499dea9a8fbafd48843f9d89970 (from https://pypi.org/simple/beautifulsoup/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "  Downloading BeautifulSoup-3.2.1.tar.gz (31 kB)\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/1e/ee/295988deca1a5a7accd783d0dfe14524867e31abb05b6c0eeceee49c759d/BeautifulSoup-3.2.1.tar.gz#sha256=6a8cb4401111e011b579c8c52a51cdab970041cc543814bbd9577a4529fe1cdb (from https://pypi.org/simple/beautifulsoup/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "  Downloading BeautifulSoup-3.2.0.tar.gz (31 kB)\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/33/fe/15326560884f20d792d3ffc7fe8f639aab88647c9d46509a240d9bfbb6b1/BeautifulSoup-3.2.0.tar.gz#sha256=0dc52d07516c1665c9dd9f0a390a7a054bfb7b147a50b2866fb116b8909dfd37 (from https://pypi.org/simple/beautifulsoup/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement beautifulSoup (from versions: 3.2.0, 3.2.1, 3.2.2)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for beautifulSoup\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "!pip install beautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1658463444355,
     "user": {
      "displayName": "집사위대한",
      "userId": "17063387912399687686"
     },
     "user_tz": -540
    },
    "id": "AsPP6bBjgApD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def driver_sleep():\n",
    "    print('.', end='')\n",
    "    time.sleep(random.uniform(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1221,
     "status": "ok",
     "timestamp": 1658463445573,
     "user": {
      "displayName": "집사위대한",
      "userId": "17063387912399687686"
     },
     "user_tz": -540
    },
    "id": "XJjIxLdea1hB"
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver as wd\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "#가상브라우저 사용\n",
    "options = wd.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"user-agent={Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36}\")\n",
    "driver = wd.Chrome('chromedriver', options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joZa2KRNh7F9"
   },
   "source": [
    "# 에브리타임 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6300,
     "status": "ok",
     "timestamp": 1658463451870,
     "user": {
      "displayName": "집사위대한",
      "userId": "17063387912399687686"
     },
     "user_tz": -540
    },
    "id": "qze7dPjZiCSc",
    "outputId": "be5a1a04-0704-438c-ec82-de99d50fb56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "base_url = 'https://everytime.kr'\n",
    "\n",
    "# 로그인 정보\n",
    "id=''\n",
    "pw=''\n",
    "\n",
    "driver.get(base_url + '/login') #페이지가 넘어갈 때마다 sleep걸어주어서 서버에 무리가 가지 않게 해야함\n",
    "\n",
    "driver_sleep()\n",
    "driver.find_element(By.NAME, 'userid').send_keys(id)\n",
    "driver.find_element(By.NAME, 'password').send_keys(pw)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"container\"]/form/p[3]/input').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QegBcOYCoKef"
   },
   "source": [
    "# 장터게시판 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXv2guLiCswq"
   },
   "source": [
    "## Step 1 - 장터게시판(원룸)으로 이동하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5422,
     "status": "ok",
     "timestamp": 1658463457291,
     "user": {
      "displayName": "집사위대한",
      "userId": "17063387912399687686"
     },
     "user_tz": -540
    },
    "id": "xi5nMYsRdLpr",
    "outputId": "62fbf393-ac72-42c1-a4c9-6d5c8255e3bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..장터게시판(원룸) 이동 완료!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver.find_element(By.XPATH, \"//a[contains(., '장터게시판')]\").click()\n",
    "driver_sleep()\n",
    "\n",
    "driver.find_element(By.XPATH, \"//div[./span[text()='원룸']]\").click()\n",
    "driver_sleep()\n",
    "\n",
    "print(\"장터게시판(원룸) 이동 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTf5XcH6DgpS"
   },
   "source": [
    "## Step 2 - 게시글 링크 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LezVNEnDgwi",
    "outputId": "9ddac309-647d-4c42-8666-665800ea1309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................"
     ]
    }
   ],
   "source": [
    "# 장터게시판 - 원룸\n",
    "links = []\n",
    "while True:\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # check if last page\n",
    "    if len(soup.select('a.next')) == 0:\n",
    "        break\n",
    "\n",
    "    # get articles link\n",
    "    posts = soup.select('article > a.article')\n",
    "    links.extend([(base_url + post['href']) for post in posts])\n",
    "\n",
    "    # press next page\n",
    "    driver.find_element(By.CSS_SELECTOR, '#container > div.wrap.articles > div.pagination > a.next').click()\n",
    "    driver_sleep()\n",
    "\n",
    "print(\"장터게시판(원룸) 게시글 링크 크롤링 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4pY4ZTJCp9h"
   },
   "source": [
    "## Step 3 - 게시글 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WJlvOPv3CoFO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................장터게시판(원룸) 게시글 데이터 크롤링 완료!\n"
     ]
    }
   ],
   "source": [
    "article_list = []\n",
    "for i in range(len(links)):\n",
    "    driver.get(links[i])\n",
    "    driver_sleep()\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        date = soup.select_one('time').text\n",
    "        title = soup.select_one('h2.large').text\n",
    "        texts = soup.select('p.large')\n",
    "        content = texts[0].text\n",
    "        comments = [text.text for text in texts[1:]]\n",
    "    except Exception as e:\n",
    "        date = ''\n",
    "        title = ''\n",
    "        content = ''\n",
    "        comments = []\n",
    "\n",
    "    article_list.append([\n",
    "        i+1,\n",
    "        '서울대학교',\n",
    "        date,\n",
    "        title,\n",
    "        content,\n",
    "        len(comments),\n",
    "        '\\n'.join(comments),\n",
    "        links[i]\n",
    "    ])\n",
    "print(\"장터게시판(원룸) 게시글 데이터 크롤링 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_6nV5OQOJSN"
   },
   "source": [
    "## Step 4 - 엑셀에 데이터 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggwYgN9UCjM4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYZFn9dfOPBA"
   },
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "import pandas as pd\n",
    "\n",
    "wb = Workbook(write_only = True)\n",
    "ws = wb.create_sheet('서울대학교')\n",
    "ws.append(['번호', '학교', '일시', '제목', '본문', '댓글 수', '댓글', '링크'])\n",
    "\n",
    "for article in article_list:\n",
    "    ws.append(article)\n",
    "\n",
    "wb.save('./에브리타임 크롤링.xlsx')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75X7tlusQDYT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Everytime.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
